\chapter{Conclusion}
\label{chp:conclusion}

\section{Summary}

This dissertation set out to alleviate the state of scholarly data. In order to achieve this, the following research objective was set.

\begin{infobox-objective}
\textbf{Research Objective}\\
Develop an automated process that takes as input scientific publications, and produces as output a high-quality derivative representation of the publications, suitable for digital processing.
\end{infobox-objective}

Criteria for high-quality were defined across the following dimensions.\footnote{See Table~\ref{tab:scholdataquali} in Chapter~\ref{chp:foundations} for the list of criteria.}

\begin{infobox-progress}
      \textbf{Data Quality Dimensions}\\
       (1)~relevance, (2)~accuracy, (3)~timeliness, (4)~comparability, (5)~completeness
\end{infobox-progress}

To focus the efforts of improving scholarly data quality, we identified three areas in which scholarly data based on existing work particularly has limitations (see also Section \ref{sec:intro-gap}).

\begin{enumerate}
    \item \textbf{Citation Network}\\
          Lacking completeness of the network connecting publications through citations
    \item \textbf{Anglocentrism}\\
          Lacking coverage of non-English publications in data used and analyzed
    \item \textbf{Research Artifacts}\\
           Lack of structured representation of research artifacts mentioned in publications
\end{enumerate}

Based on these, four research tasks were defined.

\begin{rtlist}
    \item \textit{Base Methodology} - identify or establish a base methodology for generating a large-scale, high quality scholarly data set, that is on par with or improving upon existing data sets.
    \item \textit{Citation Network Completeness} - develop a method to link literature references, that is able to link more references than are linked in existing data sets, while not compromising on link correctness or processing efficiency.
    \item \textit{Inclusion of Non-English Publications} - find and implement an approach to to include non-English publications into a large-scale, high quality scholarly data set.
    \item \textit{Fine-gained Research Artifact Representations} - develop a method to extract fine-grained information on research artifacts from text in scientific publications.
\end{rtlist}

To achieve improvements in these areas, the following steps were performed. In Chapter~\ref{chp:corpus} we developed a corpus creation method transforming publications' \LaTeX\ source files into a large-scale corpus of interlinked, annotated, full-text documents (\rtmark{1}). The presented method also includes a highly accurate reference matching procedure (\rtmark{2}). Applying our method on the complete set of all publications on arXiv.org, we created the data set \emph{unarXive}, which was used as a basis for all subsequent work. %``Corpus''
In Chapter~\ref{chp:covgran} we presented improvements regarding the citation network and the granularity of document representations. For the citation network, a blocking technique was developed that, applied on the set of references in a corpus, increases the number of matched references and bibliographic couplings. With an updated corpus creation method, the \emph{unarXive} data set achieved a more complete, state-of-the-art citation network (\rtmark{2}). The updated procedure furthermore enabled more fine-granular document representations, which in turn made the subsequent work in Chapter~\ref{chp:params} possible. %``Reference Coverage and Granularity''
In Chapter~\ref{chp:xling} we studied cross-lingual citations in the \emph{unarXive} corpus. For this, we developed a method to reliably identify this type of citation based on raw reference strings. In our study, which is the largest of its kind to date, we analysed cross-lingual citations' prevalence, usage, and impact (\rtmark{3}). %``References Across Languages''
Lastly, in Chapter~\ref{chp:params} we developed methods for extracting information about research artifacts and their usage parameters from publication full-texts. Applying our best performing method on \emph{unarXive}, we found differences in parameter reporting patterns across several disciplines (\rtmark{4}). %``References with Usage Parameters''

The contributions towards the overall research objective that are made across work on the four research tasks is summarized in the overview below. As can be seen, improvements are made across all quality dimensions and criteria. Details regarding each of the contributions are given in the respective chapters.

\begin{infobox-progress}
      \textbf{Scholarly Data Quality Contributions - Overview}\vspace{0.5em}

      \begin{tabular}{llcccc}
        \toprule
        Quality Dimension & Criterion\hphantom{mmm}& \multicolumn{4}{c}{Contribution} \\
        \midrule
        \multirow{2}{*}{Relevance} & $\mathbf{Rel_{CN}}$ & {\large\textbf{+}} & = & {\large\textbf{+}} & $\circ$ \\
         & $\mathbf{Rel_{SDR}}$ & $\circ$ & {\large\textbf{+}} & $\circ$ & {\large\textbf{+}} \\
        \arrayrulecolor{lightgrey}\hline\arrayrulecolor{black}
        \multirow{2}{*}{Accuracy} & $\mathbf{Acc_{CN}}$ & {\large\textbf{+}} & = & $\circ$ & $\circ$ \\
         & $\mathbf{Acc_{SDR}}$ & {\large\textbf{+}} & = & $\circ$ & $\circ$ \\
        \arrayrulecolor{lightgrey}\hline\arrayrulecolor{black}
        Timeliness & $\mathbf{Tim_{C/S}}$ & {\large\textbf{+}} & {\large\textbf{+}} & $\circ$ & $\circ$ \\
        \arrayrulecolor{lightgrey}\hline\arrayrulecolor{black}
        \multirow{2}{*}{Comparability} & $\mathbf{Coy_{CN}}$ & {\large\textbf{+}} & {\large\textbf{+}} & {\large\textbf{+}} & $\circ$ \\
         & $\mathbf{Coy_{SDR}}$ & $\circ$ & {\large\textbf{+}} & $\circ$ & {\large\textbf{+}} \\
        \arrayrulecolor{lightgrey}\hline\arrayrulecolor{black}
        \multirow{2}{*}{Completeness} & $\mathbf{Cos_{CN}}$ & {\large\textbf{+}} & {\large\textbf{+}} & $\circ$ & $\circ$ \\
         & $\mathbf{Cos_{SDR}}$ & {\large\textbf{+}} & {\large\textbf{+}} & $\circ$ & $\circ$ \\
        \midrule
        \midrule
        \multicolumn{2}{r}{Chapter} & \ref{chp:corpus} & \ref{chp:covgran} & \ref{chp:xling} & \ref{chp:params} \\
        \multicolumn{2}{r}{Publication} & \cite{Saier2020} & \cite{Saier2022ULITE,Saier2023unarXive} & \cite{Saier2020xling,Saier2021} & \cite{Saier2023hyperpie} \\
        \bottomrule
      \end{tabular}

      \vspace{0.5em}
      \begin{footnotesize}
      \textbf{Legend}\\
      \textbf{+}: SOTA/improvement/etc. (see respective chapter)\\
      =: equal to previous\\
      $\circ$: not considered.
      \end{footnotesize}
\end{infobox-progress}

\section{Discussion}

Structured representations of scientific publications bear large potential for accelerated progress and well founded strategic decisions in academia. Their use furthermore can mitigate the information overflow that scientists are faced with.
Our current ``digital record of science'', however, is limited in various ways. This not only prevents faster progress, but also means decisions based on current scholarly data can be flawed and analysis results faulty.
Particularly, the \emph{citation networks} in scholarly data are incomplete, \emph{non-English content} is often not covered, and increasingly relevant \emph{research artifacts} are not available as structured data.

In this dissertation, we made significant progress in all three aforementioned areas, as laid out in the previous section. With this, we achieved comprehensive improvements of data quality, as determined across the five dimensions relevance, accuracy, timeliness, comparability, and completeness. Furthermore, the presented work already made an impact on the research fields concerned with scholarly data and the study of publications. Below, we give a brief account of ideas and results from this dissertation permeating into and being used in the research community.

\begin{itemize}
    \item Use of \textbf{methodology}
    \begin{itemize}
        \item In~\cite{Lo2020} Lo et al.\ use our corpus creation methodology for creating the \LaTeX\ subset of their S2ORC data set.
        \item Chen et al.\ extend upon our reference matching procedure in~\cite{Chen2021} for the creation of their SciXGen data set.
    \end{itemize}
    \item Use for \textbf{model development and evaluation}
    \begin{itemize}
        \item Meyer et al.\ use our data for the development and evaluation of a citation recommendation model in~\cite{Citcom2021}. %~(\cite{Saier2019,HybridCite2020})
        \item In~\cite{Parisot2022} Parisot and Zavrel train a novel multi-objective representation learning technique for scientific document retrieval on our data.
        \item With Researcher2Vec Mochihashi present a method for researcher profile embeddings in~\cite{Mochihashi2023}, using our data to validate their approach.
        %\item Reference Linking~(\cite{Saier2022ULITE})
    \end{itemize}
    \item Use for \textbf{analyses}
    \begin{itemize}
        \item In~\cite{Veneri2022} Veneri et al.\ use our data to investigate how astronomers cite other research fields.
        \item Xue uses our data to analyse in~\cite{Xue2021} semantic shifts of the contexts in which works are cited. %~(\cite{Saier2020xling,Saier2021})
        \item Meng et al.\ use our data in~\cite{Meng2023} for an analysis of omitted citations of works that have become common knowledge --- so called ``obliteration by incorporation''.
    \end{itemize}
    % \item Use for \textbf{data set integration/extension}
    %\begin{itemize}  % TODO: add if Chen paper is ready in time
    %    \item Link Prediction~(\cite{Saier2023cocon})
    %    %\item NER+RE~(\cite{Saier2023hyperpie})
    %\end{itemize}
\end{itemize}

Compared to other efforts with the goal to create high-quality scholarly data, our work particularly stands out through the combination of three key aspects. (1) Accurate, fine-granular document representations, (2) a citation network, and (3) applicability on a large scale due to being automated. The closest related work, S2ORC~\cite{Lo2020}, predominantly uses PDF data and therefore does not provide the same level of granularity (e.g. mathematical notation) and is more prone to noise. arXMLiv~\cite{arXMLiv}, while providing accurate, fine-granular document representations, lacks a citation network. Lastly, the Open Research Knowledge Graph (ORKG)~\cite{orkg1,orkg2} relies on manual or only semi-automated adding of data, and is therefore lacking in scale.

% TODO: consider discussing arXiv being a preprint server and the level of difference between preprints on arXiv and their published counterpart

% Note that in other contexts the term ``scholarly data'' can have a broader meaning than just ``structured representations of scientific publications''. For example, it can also include data on research institutions or funding bodies, without necessitating the context of a publication.

Our work, as well as aforementioned related work, seek to represent scientific publications in a broad way. That is, multiple scientific disciplines and large time spans are covered, and the structured data reflects multiple aspects such as full-text, citation network, authors, etc. An alternative approach to high quality scholarly data could be dedicated efforts in specific areas. An example of such an effort is the OpenCitations Index\refurl{https://opencitations.net/index}{2023-11-30}, focussing solely on citation data. Such an approach, however, necessitates the ability to combine multiple dedicated resources. For example, combining citation data with publication full-texts. This is only possible as far as unique persistent identifiers for all involved entities exist---e.g. DOIs for documents, ORCiDs for authors, and ROR IDs for affiliations~\cite{Meadows2019}. Although use of such identifiers is becoming more and more established,\footnote{For example, arXiv.org began automatically assigning DOIs to all submissions in 2022, and Springer Nature trialed mandatory use of ORCiDs in 2018. See \refurlinline{https://blog.arxiv.org/2022/02/17/new-arxiv-articles-are-now-automatically-assigned-dois/}{2023-11-30} and \refurlinline{https://info.orcid.org/orcid-mandate-trial-at-springer-nature/}{2023-11-30}.} gaps in their coverage mean that, for the moment, a combination of dedicated resources is only of limited use~\cite{Youtie2017,Haak2018}.

\section{Outlook}

We conclude with a brief look at viable next developments building on the work in this dissertation, as well as potential future developments in scientific publishing in general and what they would mean for the presented work.

% extension to more input formats
Our work takes publications' \LaTeX\ source files from arXiv.org as the starting point. Because \LaTeX\ provides a certain level of explicit document structure and semantic information, a natural extension would be to replicate our work on the likewise structured JATS XML publications of the PubMed Central Open Access Subset. This would widen the scope of the results attained, namely by adding life sciences to the already covered disciplines of physics, mathematics, and computer science. In disciplines other than the aforementioned, however, only PDFs are available in large quantities given the current state of scientific publishing. An extension to PDF input would likely come with challenges regarding structured document representations. However, the work we presented for the focus areas \emph{citation network}, \emph{non-English content}, and \emph{research artifacts}---see Chapters~\ref{chp:covgran}, \ref{chp:xling} and \ref{chp:params} respectively---is not reliant on \LaTeX\ as a starting point. This means, given suitable input, application on PDF based document collections is unlikely to pose difficulties.

% methodological future steps
In terms of information extraction methodologies, a continuation of the recent advances in LLMs could enable more effective semantic enrichment of scholarly data. The application of LLMs, even though their performance is not on par with dedicated models yet~\cite{Yang2023}, is promising because models have wide-ranging background knowledge. A particular challenge with the application of LLMs for the creation of scholarly data is scaling approaches to large scale applications.

% the future of input formats
Looking beyond the current state of scholarly data, where it is necessary to apply information extraction methods to retroactively determine document structure and semantic information, future developments regarding ``tagged PDFs''\refurl{https://taggedpdf.com/}{2023-11-30} could simplify the creation of high quality scholarly data. Widespread adoption of encouragement or requirements for semantically tagging PDFs could either be driven by efforts to improve the accessibility of scientific publications, or by the fact that it would facility data mining. In STEM fields, a prerequisite for such a developments would be that the \LaTeX\ Project's plan to support semantic annotation natively succeeds~\cite{Mittelbach2020,Mittelbach2023}.

% the futer of publishing
Considering future developments of the landscape of scientific publications, changes to the status quo of papers being the primary unit of publication would accordingly bring changes to the nature of scholarly data. For example, establishment of micropublications~\cite{Raciti2018} and further adoption of data citations~\cite{Kratz2015} could bring new requirements to 
scholarly data both in terms of data modeling as well as information extraction methods.
