\chapter{Foundations}
\label{chp:foundations}

This chapter provides information on overarching as well as foundational concepts relevant to the following chapters. Specifically, we cover two areas.

\begin{enumerate}
    \item \textbf{Scholarly Data}\\
        First, we give an overview of the academic publication ecosystem and its relation to the landscape of scholarly data. Understanding the parts involved and relations between them is helpful for understanding decisions made for the system design and method development of the approaches presented later on.
    \item \textbf{Data Mining \& Information Extraction}\\
        Second, we present relevant concepts from the areas of data mining and information extraction. These are essential for a quantification of the research goals as well as the results that were achieved.
\end{enumerate}

Explanations of concepts that are specific to the work presented in individual chapters, as well as an overview of state of the art approaches in the respective areas, are provided jointly with the approaches in Chapters \ref{chp:corpus}\,--\,\ref{chp:params}.

\section{Scholarly Data}

% What do I want to say and convey here?

% - what's the general nature of scholarly data?
%   -> structured representation of publication [meta]data that is the basis for (1) digital services in academia, (2) analyses, (3) ML model dev+evaluation

The term ``Scholarly data'' is used to refer to data that represents academic publications or related concepts, such as authors and their affiliations. It can coarsely be divided into data directly reflecting the \emph{content} of publications, and metadata, which gives information \emph{about} publications.
As such, scholarly data is the basis for essentially everything that relies on digital processing of publications. The following are three key examples.
\begin{enumerate}
    \item \textbf{Digital services} in academia such as search (e.g. Google Scholar\refurl{https://scholar.google.com/}{2023-11-06}, Semantic Scholar\refurl{https://www.semanticscholar.org/}{2023-11-06}), recommendation (e.g. Academia.edu\refurl{https://www.academia.edu/}{2023-11-06}, CORE Recommender\refurl{https://core.ac.uk/services/recommender}{2023-11-06}), and aggregation platforms (e.g. Papers With Code\refurl{https://paperswithcode.com/}{2023-11-06}, Scopus\refurl{https://www.scopus.com/}{2023-11-06}).  % TODO: consider adding whats bad if data incorrect
    \item \textbf{Analyses} such as bibliometric analyses across time, geographic regions, or institutions, as well as trend analyses and investigations into specific phenomena like citation inequity.
    \item \textbf{Model development} such as the training and evaluation of transformer based large language models (LLMs) as well as task specific models (e.g. for recommender systems, impact prediction, or information extraction).
\end{enumerate}

Because scholarly data is only a secondary product to the actual publications themselves, it is necessary to consider how the data comes into being.

\subsection{Origins of Scholarly Data}

% - how is scholarly data generated
%   -> to some degree manually (metadata provided by authors), everything else (structured representations of full-text, references, etc.) out of necessity automated
% - what are the data sources and their peculiarities?
%   -> (see fig 2.1; from 3 stages of publishing, overall visual first, some special treatment for some metadata (wrt. access and requiring authors to manually provide it)

\begin{figure}[bt]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/foundations/scholarly_data_lifecycle_dummy}
  \caption[Scholarly data origins]{Scholarly data origins (note: in text mention that for older publications it goes from some source to distribution on paper, and scholarly data then requires OCR on scanned documents)}
  \label{fig:foundations-datalifecycle}
\end{figure}


Figure~\ref{fig:foundations-datalifecycle} schematically shows the path of a publication from authorship to distribution, together with different stages from which scholarly data can emerge. It is essential to note that academic publications are, historically and at the time of writing still, primarily written by humans with human readership in mind. As such, publications are primarily a visual medium, optimized for parsing by human vision and intelligence. Scholarly data, however, is intended for automated processing and therefore benefits, for example, from strict syntactic rules and no reliance on assumed background knowledge. As a consequence, the creation of scholarly data requires bridging between the existing visual presentation and desired structural derivate.

Specifically, this means that information which is not made explicit in publications needs to be retroactively added. For example, if a piece of text \textit{``[1,3]''} in a publication is expressing an interval of real numbers, or a citation for references 1 and 3. Because it would be impractical to require researchers to produce a detailed set of annotations in addition to all their publications,\footnote{This can be seen as a case of the ``authoring problem'' challenging the semantic web community~\cite{Kohlhase2010}.}\textsuperscript{,}\footnote{An exception to this is basic metadata such as title, authors, and abstract, which are commonly requested to be filled into a form in plain text during the submission process of a manuscript.} the retroactive adding of information needs to be done automatically, i.e. by means of information extraction. An shown in Figure~\ref{fig:foundations-datalifecycle}, the information extraction can happen at any given stage of publication. At each stage, the nature of the available data and information is different. Accordingly, there are different benefits and challenges in each case. In the following, we will discuss these different types of data from an information extraction perspective, beginning with \LaTeX\ as it is the most relevant to the presented work.

\subsubsection{Types of Data Sources}

\begin{table}[tb]
  \caption{Comparison of Document Formats}
  \label{tab:document_formats}
  \centering
  \begin{small}
    \begin{threeparttable}
      \begin{tabular}{lcclc}
        \toprule
        Format & Doc Structure & Semantic & Open Availability & Disciplines \\
        \midrule
        LaTeX  & \checkmark    & partial\tnote{a}  & \checkmark~(arXiv)  & physics, math, CS \\
        Word   & \checkmark    & partial\tnote{b}  & $\times$~(publisher internal & all\tnote{c} \\
        JATS   & \checkmark    & \checkmark & \checkmark~(PMC OAS)   & biomedical \\
        PDF    & $\times$      & $\times$   & \checkmark~(abundant)  & all  \\
        \bottomrule
      \end{tabular}
      \begin{tablenotes}
        \item[a] todo
        \item[b] todo
        \item[c] Primarily humanities and not so much STEM fields.
      \end{tablenotes}
    \end{threeparttable}
  \end{small}
\end{table}

Viable input formats for the creation of scholarly data differ in terms of the contained information, challenges for information extraction, availability, and use.

% - LaTeX; what is it, why is it not the end-all-be-all, and what endeavours have been there so far and are on the horizon of LaTeX development wrt. more structure and semantic information?
%   -> brief history, used primarily in STEM fields, Turing complete, document classes and packages provide semantic macros that translate in to visual representation but authors can always chose to directly describe visual presentation rather than semantic meaning, some endeavours to allow for more semantics (scholarly data specific and from accessibility POV)

\paragraph{\LaTeX}
is described as \textit{``a system for typesetting documents''}~\cite{Lamport1994}, or a \textit{``widely used language for describing the logical structure of [...] documents''}~\cite{Mittelbach2023}. At its core, \LaTeX\ provides functionalities to control the visual presentation of a document. These functionalities are combined by macros, which offer authors the means to structurally and semantically describe a document (e.g. \texttt{\textbackslash title\{\}} or \texttt{\textbackslash section\{\}}). This structural and semantic information---which gets lost when the \LaTeX\ source is compiled to PDF---
% TODO: try to find an example of a common case where authors visually denote something instead of making it explicit by a fitting macro, where the result in the PDF is indistinguishable (also factoring in hyperref which makes footnote marks, references, citations, etc. clickable)
%\texttt{\textbackslash footnote\{Example\}}\footnote{Example} an author could use \texttt{\textbackslash textsuperscript\{\thefootnote\}} creates a footnote mark\textsuperscript{\thefootnote} that, in PDF output, is visually indistinguishable from what is produced by \texttt{\textbackslash} 
%\footnote{The translation of structural components into a visual presentation is dictated by the document classes and packages providing the macros. For example, a paper author placing text between \texttt{\textbackslash begin\{abstract\}} and \texttt{\textbackslash end\{abstract\}}, will find their abstract text prefaced with ``\textbf{Abstract.}'' and set with a reduced line width when using the document class \texttt{llncs}, whereas there will be no preface and an unchanged line width when using \texttt{acmart}.}
is immensely beneficial for information extraction.
However, \LaTeX\ documents are usually a mixture structural and presentation description, which introduces challenges~\cite{Stamerjohanns2008}.
While there have been efforts to establish \LaTeX\ extensions for more rigorous semantic annotation in the document source~\cite{Krieg2004,Groza2007,Bless2023}, these have not been widely adapted so far.
There are, however, efforts by The \LaTeX\ Project\refurl{https://www.latex-project.org/}{2023-11-08} itself to support semantic annotation natively in the future~\cite{Mittelbach2020,Mittelbach2023}.

Regarding at the availability of \LaTeX\ sources, arXiv.org\refurl{https://arxiv.org/}{2023-11-08} provides open access to over 2 million papers uploaded by their authors. With its origin in physics in the 1990s~\cite{Feder2021,Ginsparg2011a}, gradual adoption in mathematics in the early 2000s, and rapid growth in computer science since the 2010s~\cite{Saier2023unarXive}, it now covers significant portions of the scientific literature in aforementioned three disciplines. Given the benefits for information extraction, \LaTeX\ sources from arXiv have been used for generating scholarly data on a small scale since at least 1998~\cite{Nanba1998}. Large-scale efforts started with a focus on mathematics in 2008~\cite{Stamerjohanns2008}. Complete conversions of the papers on arXiv.org into a scholarly data corpus including a citation network are comparatively new development with the unarXive corpus~\cite{Saier2020} presented in this dissertation, as well as S2ORC~\cite{Lo2020}.
% Joanne Cohn sending around e-prints~\cite{Feder2021,Turner2012}
% June 1991 meets Paul Ginsparg who then goes on to start arXiv~\cite{Ginsparg2011a,Ginsparg2011}

% metadata dump since 2020(?) available on Kaggle~\cite{arxiv_kaggle_dataset}

% - What about other formats?
%   -> JATS is mighty, Word is XML, triple formats are used for metadata

\paragraph{Word}
documents (DOCX files\refurl{https://www.loc.gov/preservation/digital/formats/fdd/fdd000397.shtml}{2023-11-08}) are the second major data format commonly accepted by publishers for submitting manuscripts~\cite{Johnson2018stm}. Contrary to the \LaTeX\ approach of compilation from source files, documents are edited in an interactive ``What you see is what you get'' (WYSIWYG) editor.\refurl{https://www.microsoft.com/microsoft-365/word}{2023-11-08} While DOCX files are essentially ZIP compressed XML files and contain more explicit information then PDF derivates, there are no open repositories similar to arXiv.org that provide large quantities of paper's Word source files.  % open question: do publishers keep them?

\paragraph{Journal Article Tag Suite}
(JATS) is a standardized markup format for scientific publications based on XML~\cite{Huh2014}. As depicted in Figure~\ref{fig:foundations-datalifecycle}, JATS files are not directly created by researchers, but are rather an intermediate format used by publishers, from which they derive different presentation formats of publications, such as PDF and HTML. While JATS files provide semantically richer information than \LaTeX\ or Word source files, their generation can only partially be automated and requires human oversight. Regarding availability, the PubMed Central Open Access Subset\refurl{https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/}{2023-11-06} (PMC OAS) provides over 3 million publications from the biomedical and life sciences domain in JATS XML format. While the JATS files of the PMC OAS have been used to generate a corpus of linked publications in the past~\cite{Gipp2015}, more up-to-date and widely used corpora have only used the PDF versions of the contained documents~\cite{Lo2020}.

\paragraph{PDF}
% TODO: find source of PDF being the most used medium for paper distribution and consumption
% (couldn't find explicit source in The STM Report)
is the most common distribution format for academic publications~\cite{Johnson2018stm} and accordingly, the largest open document collections are PDF files, such as CORE~\cite{core} with over 100 million documents. The PDF format does provide optional functionalities to describe the logical structure of documents in addition to the visual presentation~\cite{ISO32000-2}. However, such annotation is not an established practice in academic publishing and, accordingly, information extraction methods have to resort to heuristic approaches based on the visually presented information only~\cite{Lopez2009,Nasar2018,Faerber202x}. This makes PDF a more error prone source than aforementioned source formats~\cite{Bast2017}.

% \paragraph{Triple Formats}  % NOTE: note a ``source format'' from which scholarly data sets are created
% RDF, Turtle, JSON-LD, ... (relevant for metadata)

\subsubsection{Types of Scholarly Data}

\begin{table}[tb]
  \caption{Overview of Different Data Types}
  \label{tab:data_types}
  \centering
  \begin{small}
    \begin{threeparttable}
      \begin{tabular}{lclc}
        \toprule
        Data Type                  & Contains                             & Size\tnote{a} & Examples \\
        \midrule
        Metadata                   & Title, Author, Citations, etc.       & \(10^8\)              & OpenAlex, ORKG \\
        Document Collections       & Full-text                            & \(10^8\)              & CORE, arXMLiv \\
        Linked Doc. Collections    & Full-text + citation network         & \(10^7\)              & unarXive, S2ORC \\
        \bottomrule
      \end{tabular}
      \begin{tablenotes}
        \item[a] Order of publications covered by large representatives of each type
      \end{tablenotes}
    \end{threeparttable}
  \end{small}
\end{table}

Conceptually, scholarly data can be divided two overarching categories: \emph{metadata} and \emph{document collections}~\cite{Nasar2018}. As a third category, we consider \emph{lined document collections}, which combine features of aforementioned two.

% What do I want to say and convey here?
% - what ``conceptual'' types of scholarly data are there?
%   -> metadata, document collections, linked document collections (combining former two b/c metadata usually also means citations)
% - what are current/established (``SOTA'') approaches/endeavors/projects
%   -> ORKG (very semantic but not scalable yet), arXMLiv (math focus), S2ORC (PDF source), OpenAlex (very large, ...)

\paragraph{Metadata}
provides information \emph{about} publications, rather than reflecting their full-text content. The data partially originates in already structured form provided by authors, as it is queried by publishers during manuscript submission. This includes the title, authors, as well as the abstract. Different regarding the data's origin are bibliographic references, which are also often included in metadata sets. Here, it is necessary to extract the reference information from the document submitted by an author (e.g. \LaTeX\ or Word sources). Regarding accessibility, title and author information is generally shared freely. Abstracts and references may also be openly acceptable, but this is not always the case, as evidenced by the existence of the Initiative for Open Abstracts\refurl{https://i4oa.org/}{2023-11-09} and the Initiative for Open Citations.\refurl{https://i4oc.org/}


Two examples of scholarly metadata sets are the following. (1)~OpenAlex~\cite{openalex} contains data on over 200\,M publications, their authors, affiliations, citation links, etc. Its data sources include PubMed, arXiv, academic publishers, and various institutional repositories.%\refurl{https://api.openalex.org/sources}{2023-11-09} 
 (2)~The Open Research Knowledge Graph (ORKG)~\cite{orkg1,orkg2} provides information on over 28\,k\footnote{28.020 entities of type \texttt{http://orkg.org/orkg/class/Paper} as of 2023-11-09. Determined via the ORKG SPARQL endpoint at \url{https://www.orkg.org/sparql}.} publications, their contributions, research problems addressed etc. For its data, the ORKG is largely reliant on manual or semi-automated data entry.

\paragraph{Document Collections} provide access to publications' full-text content. Because of this, they are reliant on open access publications as a data source. Furthermore, the documents either need to be licensed in a way that allows (re-)distribution, or access to the collection itself needs to be restricted accordingly. Because there are documents sources in different formats (see Figure~\ref{fig:foundations-datalifecycle}), document collections derived from them take different forms. While some are primarily aggregates of openly accessible PDFs(and therefore can also be seen as a data source), such as CORE~\cite{core}, others are the result of an involved generation process. An example for the latter is arXMLiv~\cite{arXMLiv}, which is an XML conversion of the papers on arXiv, comprising 1.6 M documents in its most recent version.

In order to use a document collection for applications involving citation information, the it has to be jointly used with a suitable set of citation metadata. This requires (1)~the citation metadata to cover the documents within the collection, and (2)~the availability of identifiers (such as DOIs) or some matching procedure.  % NOTE: could also mention that citations information is then limited to that among the documents in the collection. *not* covered are all citations where the collection documents are the citing paper --- b/c citation relations aren't that ``self contained''

\paragraph{Linked Document Collections}
are document collections that, in addition to the document contents, include a citation network. This means, that in addition to the requirements for creating a document collection, their generation involves the additional step of linking the references in the documents' reference sections. Commonly, this furthermore involves linking in-text citations to their respective references, to provide for more fine-grained citation information.

Two representatives of linked document collections are the following. (1)~unarXive~\cite{Saier2020} is generated from the \LaTeX\ sources on arXiv, comprising 2 M documents in its most recent version. Being generated from arXiv, it covers physics, mathematics, and computer science publications. S2ORC~\cite{Lo2020} is generated from PDF files of various sources such as PubMed and publishers' websites, and contains over 12 M documents. The publications covered are primarily from die fields of medicine and biology, followed by physics, mathematics, and computer science.

\section{Data Mining \& Information Extraction}

% What do I want to say and convey here?
% - 

% ``Data Mining is the process of discovering useful patterns and trends in large data sets.''~\cite{Larose2014}
% Information Extraction the process of ``converting unstructured text into a structured representation''~\cite{Aggarwal2018}

% (failed) attempt at a segue: The creation of large-scale scholarly data, as described in the previous section, requires the processing of some form of source data. ...

The work presented in this dissertation involves both data mining, the process of ``discovering useful patterns and trends in large data sets''~\cite{Larose2014}, and information extraction, the process of ``converting unstructured text into a structured representation''~\cite{Aggarwal2018}. In the following, we therefore introduce relevant concepts with respect to (1)~data quality, and (2)~the evaluation of information extraction models.

\subsection{Data Quality}

% - mby sync w/ four quality criteria

\subsection{Model Evaluation}

Model evaluation is what's guiding the development and assessment of approaches to computational problems. For example, when developing a machine learning approach to a classification problem, it is of interest to compare the approach to existing ones. In essence, a model evaluation results in a performance estimate. This is because the model performance can not be known in advance for every possible input, so only an estimate based on a set of realistic inputs can be attained.
To get a performance estimate, the model is applied on inputs, for which the desired output is already known---a \emph{ground truth}. The model outputs are then compared with the desired outputs to by means of \emph{evaluation metrics}. In the following, we describe the means of such output comparisons, as well as several evaluation metrics.

\subsubsection{Basic Concepts}

\paragraph{Confusion Matrix}

\begin{equation*}
\begin{array}{cc|cc}
    & \ & \multicolumn{2}{c}{\text{Actual}} \\
    & \ & \text{Positive} & \text{Negative} \\
    \hline
    \multirow{2}{*}{Predicted} & \text{Positive} & \text{True Positives (TP)} & \text{False Positives (FP)} \\
     & \text{Negative} & \text{False Negatives (FN)} & \text{True Negatives (TN)} \\
\end{array}
\end{equation*}


\paragraph{Accuracy}

\begin{equation*}
\text{Accuracy (ACC)} = \frac{\text{TP + TN}}{\text{TP + TN + FP + FN}}
\end{equation*}

\paragraph{Precision}

\begin{equation*}
\text{Precision (P)} = \frac{\text{TP}}{\text{TP + FP}}
\end{equation*}


\paragraph{Recall}

\begin{equation*}
\text{Recall (R)} = \frac{\text{TP}}{\text{TP + FN}}
\end{equation*}

\paragraph{F-measure}

\begin{equation*}
\text{F1-Score (F1)} = \frac{2 \cdot \text{P} \cdot \text{R}}{\text{P + R}}
\end{equation*}

% part | task | metrics
% - unarXive20 | parsing | success rate
% - unarXive20 | matching->multiclass clf | success rate, accuracy? (mby precision)
% - blocking | matching->multiclass clf | p, r, f1
% - unarXive22 | matching | success rate
% - xling | language detection->clf | p, r, f1
% - hyperpie | ner->seq. labeling->clf| p, r, f1
% - hyperpie | re->binary clf | p, r, f1

\subsubsection{Task Specific Considerations}

\paragraph{Reference Linking}

\paragraph{Entity Recognition}

\paragraph{Relation Extraction}

% - reference linking
%   - can only check if predicted link is correct (TP/TP+FP = prection)
%   - can't check if no match is b/c not in target collection or really missed match (no FN/TN info)
% - NER
%   - 
% - RE
%   - class imbalance
%   - tricky eval when modeling with entities that have multiple mentions/surface forms
